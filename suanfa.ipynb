{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, CLIPVisionModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import warnings\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# å¿½ç•¥ PEFT å’Œ HUGGINGFACE çš„ä¸€äº›è­¦å‘Š\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# è®¾å¤‡ä¸ç²¾åº¦é…ç½®\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. è§†è§‰ç‰¹å¾ Adapter\n",
    "\n",
    "class VisionAdapter(nn.Module):\n",
    "    \"\"\"å°† CLIP ç‰¹å¾æ˜ å°„åˆ° LLM ç©ºé—´å¹¶ç”Ÿæˆå¯æ³¨å…¥çš„ KV tokensã€‚\"\"\"\n",
    "\n",
    "    def __init__(self, clip_dim: int, llm_dim: int, num_prefix_tokens: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_prefix_tokens = num_prefix_tokens\n",
    "\n",
    "        self.projection = nn.Linear(clip_dim, llm_dim)\n",
    "        self.prefix_tokens = nn.Parameter(torch.randn(1, num_prefix_tokens, llm_dim))\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.LayerNorm(llm_dim),\n",
    "            nn.Linear(llm_dim, llm_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(llm_dim // 2, llm_dim),\n",
    "        )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "    def forward(self, clip_features: torch.Tensor):\n",
    "        mapped_features = self.projection(clip_features)\n",
    "        bsz = mapped_features.shape[0]\n",
    "        adapter_input = torch.cat(\n",
    "            [self.prefix_tokens.repeat(bsz, 1, 1), mapped_features], dim=1\n",
    "        )\n",
    "        return self.adapter_block(adapter_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ”¹é€ çš„ ChatGLM3 Attention\n",
    "\n",
    "class ModifiedChatGLMAttention(nn.Module):\n",
    "    \"\"\"ç¤ºæ„å¦‚ä½•åœ¨ç‰¹å®šå±‚æ³¨å…¥è§†è§‰ KVã€‚çœŸå®ç¯å¢ƒéœ€æ”¹åŠ¨æºç ã€‚\"\"\"\n",
    "\n",
    "    def __init__(self, original_attention_module, config, layer_idx, adapter_dim, num_layers=28):\n",
    "        super().__init__()\n",
    "        self.original_attention = original_attention_module\n",
    "        self.inject_layers = set(\n",
    "            [int(num_layers * 0.1), int(num_layers * 0.3), int(num_layers * 0.6), int(num_layers * 0.9)]\n",
    "        )\n",
    "        self.vision_adapter = None\n",
    "        if layer_idx in self.inject_layers:\n",
    "            self.vision_adapter = VisionAdapter(clip_dim=adapter_dim, llm_dim=config.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, adapter_output=None, *args, **kwargs):\n",
    "        k_vis = v_vis = None\n",
    "        if self.vision_adapter and adapter_output is not None:\n",
    "            adapter_kv_tokens = self.vision_adapter(adapter_output)\n",
    "            k_vis = adapter_kv_tokens\n",
    "            v_vis = adapter_kv_tokens\n",
    "\n",
    "        return self.original_attention(\n",
    "            hidden_states,\n",
    "            K_vis=k_vis,\n",
    "            V_vis=v_vis,\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¤šæ¨¡æ€æ¨¡å‹å°è£…\n",
    "\n",
    "class ChatGLM3ForMultimodal(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_path=\"THUDM/chatglm3-6b\",\n",
    "        clip_model_path=\"openai/clip-vit-base-patch16\",\n",
    "        torch_dtype=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = get_device()\n",
    "        self.torch_dtype = torch_dtype or (torch.float16 if self.device.type == \"cuda\" else torch.float32)\n",
    "\n",
    "        # å†»ç»“çš„ CLIP è§†è§‰ç¼–ç å™¨ï¼ˆæ”¾åœ¨ç›®æ ‡ dtype ä¸Šï¼Œå‡å°æ˜¾å­˜ï¼‰\n",
    "        self.clip = CLIPVisionModel.from_pretrained(clip_model_path, torch_dtype=self.torch_dtype)\n",
    "        self.clip.requires_grad_(False)\n",
    "        self.clip = self.clip.to(self.device)\n",
    "        self.clip_dim = self.clip.config.hidden_size\n",
    "\n",
    "        # LLM + LoRA\n",
    "        self.llm = AutoModel.from_pretrained(llm_model_path, trust_remote_code=True, torch_dtype=self.torch_dtype)\n",
    "        llm_config = self.llm.config\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"query_key_value\", \"dense\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        self.llm = get_peft_model(self.llm, lora_config).to(self.device)\n",
    "\n",
    "        # æç¤ºï¼šå®é™…æ›¿æ¢æ³¨æ„åŠ›éœ€ä¿®æ”¹æºç ï¼Œè¿™é‡Œä»…ç¤ºæ„\n",
    "        # for i, block in enumerate(self.llm.transformer.layers):\n",
    "        #     original_attention = block.self_attention\n",
    "        #     block.self_attention = ModifiedChatGLMAttention(original_attention, llm_config, i, self.clip_dim)\n",
    "\n",
    "        self.llm.print_trainable_parameters()\n",
    "\n",
    "    def forward(self, images: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels=None):\n",
    "        images = images.to(self.device, dtype=self.torch_dtype)\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        if labels is not None:\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            clip_outputs = self.clip(images)\n",
    "            visual_features = clip_outputs.last_hidden_state\n",
    "\n",
    "        # è‹¥ä½¿ç”¨ GPUï¼Œåˆ™å¯ç”¨ autocast ä»¥é™ä½æ˜¾å­˜å¹¶åŠ é€Ÿ\n",
    "        amp_enabled = self.device.type == \"cuda\"\n",
    "        with autocast(device_type=self.device.type, enabled=amp_enabled):\n",
    "            llm_outputs = self.llm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                visual_features=visual_features,\n",
    "                labels=labels,\n",
    "            )\n",
    "        return llm_outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ç¤ºä¾‹è¿è¡Œï¼ˆè½»é‡åŒ–ç¤ºä¾‹ï¼Œè‡ªåŠ¨é€‰æ‹©è®¾å¤‡ï¼‰\n",
    "\n",
    "def example_run():\n",
    "    device = get_device()\n",
    "    torch_dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "    # å° batch / åºåˆ—ï¼Œæ–¹ä¾¿æœ¬åœ°å¿«é€Ÿ smoke test\n",
    "    bsz, seq_len = 1, 32\n",
    "    model = ChatGLM3ForMultimodal(torch_dtype=torch_dtype)\n",
    "\n",
    "    images = torch.randn(bsz, 3, 224, 224, device=device, dtype=torch_dtype)\n",
    "    input_ids = torch.randint(0, 50000, (bsz, seq_len), device=device)\n",
    "    attention_mask = torch.ones(bsz, seq_len, device=device)\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    model.train()\n",
    "    print(\"\\n--- è¿è¡Œ Forward Pass ---\")\n",
    "    try:\n",
    "        outputs = model(images, input_ids, attention_mask, labels=labels)\n",
    "        print(f\"logits å½¢çŠ¶: {outputs.logits.shape}\")\n",
    "        if outputs.loss is not None:\n",
    "            print(f\"loss: {outputs.loss.item():.4f}\")\n",
    "        print(\"âœ… Forward Pass æˆåŠŸï¼ˆå‡è®¾ ChatGLM3 æºç å·²æ”¯æŒ visual_featuresï¼‰\")\n",
    "    except AttributeError as e:\n",
    "        print(\"\\nğŸš¨ éœ€è¦ä¿®æ”¹ transformers/models/chatglm3/modeling_chatglm.py ä»¥æ”¯æŒ visual_features å‚æ•°ã€‚\")\n",
    "        print(f\"åŸå§‹é”™è¯¯: {e}\")\n",
    "\n",
    "    print(\"\\n--- å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡ ---\")\n",
    "    model.llm.print_trainable_parameters()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
